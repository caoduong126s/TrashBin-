{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 4 - STEP 4.2: EFFICIENTNET-B0 TRAINING\n",
    "\n",
    "**Objective:** Train EfficientNet-B0 with transfer learning for improved accuracy\n",
    "\n",
    "**Expected Results:**\n",
    "- Training time: ~2-3 hours (with MPS) or ~4-6 hours (CPU)\n",
    "- Validation accuracy: ~88-92%\n",
    "- Significant improvement over baseline (+15-20%)\n",
    "\n",
    "**Strategy:**\n",
    "- Phase 1: Freeze base model, train top layers (~15 epochs)\n",
    "- Phase 2: Unfreeze, fine-tune entire model (~40 epochs)\n",
    "- Total: ~55 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTS & SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "Keras version: 3.x (integrated with TensorFlow 2.15.0)\n",
      "GPU available: True\n",
      "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "âœ… Running on Apple Silicon with Metal acceleration\n",
      "\n",
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    ")\n",
    "\n",
    "# Print versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Check Keras version (Keras 3 doesn't have __version__)\n",
    "try:\n",
    "    print(f\"Keras version: {keras.__version__}\")\n",
    "except AttributeError:\n",
    "    print(f\"Keras version: 3.x (integrated with TensorFlow {tf.__version__})\")\n",
    "\n",
    "# Check GPU/Metal availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU available: {len(gpus) > 0}\")\n",
    "if len(gpus) > 0:\n",
    "    print(f\"GPU devices: {gpus}\")\n",
    "    print(f\"âœ… Running on Apple Silicon with Metal acceleration\")\n",
    "else:\n",
    "    print(f\"Running on CPU only\")\n",
    "\n",
    "# Set plot style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except:\n",
    "        plt.style.use('default')\n",
    "        print(\"âš ï¸  Seaborn style not available, using default\")\n",
    "\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\nâœ… All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Image size: 224Ã—224\n",
      "  Batch size: 32\n",
      "  Phase 1 epochs: 15 (freeze base)\n",
      "  Phase 2 epochs: 40 (fine-tune)\n",
      "  Total max epochs: 55\n",
      "  Classes: 9\n",
      "\n",
      "âœ… Configuration set!\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path('../data/processed')\n",
    "MODEL_DIR = Path('../models')\n",
    "OUTPUT_DIR = Path('../outputs')\n",
    "\n",
    "# Create directories\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_PHASE1 = 15  # Freeze base, train top\n",
    "EPOCHS_PHASE2 = 40  # Fine-tune entire model\n",
    "LEARNING_RATE_PHASE1 = 0.001\n",
    "LEARNING_RATE_PHASE2 = 0.0001  # 10Ã— lower for fine-tuning\n",
    "\n",
    "# Class names\n",
    "CLASS_NAMES = ['battery', 'biological', 'cardboard', 'glass', \n",
    "               'metal', 'paper', 'plastic', 'textile', 'trash']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Image size: {IMG_SIZE}Ã—{IMG_SIZE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Phase 1 epochs: {EPOCHS_PHASE1} (freeze base)\")\n",
    "print(f\"  Phase 2 epochs: {EPOCHS_PHASE2} (fine-tune)\")\n",
    "print(f\"  Total max epochs: {EPOCHS_PHASE1 + EPOCHS_PHASE2}\")\n",
    "print(f\"  Classes: {NUM_CLASSES}\")\n",
    "print(f\"\\nâœ… Configuration set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LOAD CLASS WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights loaded:\n",
      "  battery     : 1.8245\n",
      "  biological  : 1.7504\n",
      "  cardboard   : 1.9358\n",
      "  glass       : 0.8578\n",
      "  metal       : 2.2416\n",
      "  paper       : 1.6408\n",
      "  plastic     : 1.9934\n",
      "  textile     : 0.2360\n",
      "  trash       : 2.4764\n",
      "\n",
      " Class weights loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load class weights\n",
    "weights_file = DATA_DIR / 'class_weights_simple.json'\n",
    "\n",
    "with open(weights_file, 'r') as f:\n",
    "    weights_data = json.load(f)\n",
    "\n",
    "# Convert to format needed by Keras\n",
    "class_weights = {int(k): v for k, v in weights_data['recommended_weights'].items()}\n",
    "\n",
    "print(\"Class weights loaded:\")\n",
    "for class_idx, weight in class_weights.items():\n",
    "    print(f\"  {CLASS_NAMES[class_idx]:12s}: {weight:.4f}\")\n",
    "\n",
    "print(\"\\n Class weights loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CREATE DATA GENERATORS (STRONGER AUGMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating data generators...\\n\")\n",
    "\n",
    "# Training generator - STRONGER augmentation than baseline\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,           # More rotation\n",
    "    width_shift_range=0.25,      # More shift\n",
    "    height_shift_range=0.25,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,          # Add vertical flip\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation generator - NO augmentation\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATA_DIR / 'train',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    DATA_DIR / 'val',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining steps per epoch: {len(train_generator)}\")\n",
    "print(f\"Validation steps per epoch: {len(val_generator)}\")\n",
    "print(\"\\nâœ… Data generators created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BUILD EFFICIENTNET-B0 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building EfficientNet-B0 model...\\n\")\n",
    "\n",
    "# Load pre-trained EfficientNet-B0\n",
    "base_model = EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',  # Pre-trained on ImageNet\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling=None\n",
    ")\n",
    "\n",
    "print(f\"âœ… Base model loaded: EfficientNet-B0\")\n",
    "print(f\"   Pre-trained on: ImageNet\")\n",
    "print(f\"   Base parameters: {base_model.count_params():,}\")\n",
    "\n",
    "# Freeze base model initially (Phase 1)\n",
    "base_model.trainable = False\n",
    "print(f\"   Status: FROZEN (will train in Phase 2)\")\n",
    "\n",
    "# Build complete model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "], name='EfficientNetB0_WasteClassification')\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total parameters:     {model.count_params():,}\")\n",
    "trainable = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "non_trainable = sum([tf.size(w).numpy() for w in model.non_trainable_weights])\n",
    "print(f\"Trainable (Phase 1):  {trainable:,}\")\n",
    "print(f\"Non-trainable:        {non_trainable:,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… Model built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COMPILE MODEL - PHASE 1 (WARMUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compiling model for Phase 1 (Warmup)...\\n\")\n",
    "\n",
    "# Use legacy optimizer for Mac M4 compatibility\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE_PHASE1),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… Model compiled!\")\n",
    "print(f\"   Phase: 1 (Warmup - Train top layers only)\")\n",
    "print(f\"   Optimizer: Adam (legacy, optimized for M4)\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE_PHASE1}\")\n",
    "print(f\"   Loss: Categorical Crossentropy\")\n",
    "print(f\"   Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SETUP CALLBACKS - PHASE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up callbacks for Phase 1...\\n\")\n",
    "\n",
    "# Callbacks for Phase 1\n",
    "callbacks_phase1 = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=str(MODEL_DIR / 'efficientnet_b0_phase1.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=8,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=0.5,\n",
    "        patience=4,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    CSVLogger(\n",
    "        str(OUTPUT_DIR / 'efficientnet_phase1_log.csv'),\n",
    "        append=True\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ… Callbacks configured:\")\n",
    "print(\"   - ModelCheckpoint: Save best model\")\n",
    "print(\"   - EarlyStopping: patience=8\")\n",
    "print(\"   - ReduceLROnPlateau: patience=4, factor=0.5\")\n",
    "print(\"   - CSVLogger: Log training metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TRAIN - PHASE 1 (WARMUP: ~15 EPOCHS, ~20-30 MIN)\n",
    "\n",
    "**What's happening:**\n",
    "- Base EfficientNet layers are FROZEN\n",
    "- Only training the custom top layers (Dense, Dropout, BatchNorm)\n",
    "- This adapts the output to our 9 waste classes\n",
    "- Expected time: ~20-30 minutes with MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PHASE 1: WARMUP TRAINING (FREEZE BASE, TRAIN TOP)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Base model: FROZEN\")\n",
    "print(f\"Training: Top layers only\")\n",
    "print(f\"Max epochs: {EPOCHS_PHASE1}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE_PHASE1}\")\n",
    "print(f\"\\nExpected time: ~20-30 minutes with MPS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸš€ Starting Phase 1 training...\\n\")\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_phase1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… PHASE 1 COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best val accuracy: {max(history_phase1.history['val_accuracy']):.4f}\")\n",
    "print(\"\\nModel saved: efficientnet_b0_phase1.h5\")\n",
    "print(\"\\nReady for Phase 2: Fine-tuning entire model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PREPARE FOR PHASE 2 (FINE-TUNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PREPARING PHASE 2: FINE-TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "print(f\"\\nâœ… Base model UNFROZEN\")\n",
    "print(f\"   All {len(base_model.layers)} layers now trainable\")\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "non_trainable = sum([tf.size(w).numpy() for w in model.non_trainable_weights])\n",
    "\n",
    "print(f\"\\nModel statistics (Phase 2):\")\n",
    "print(f\"  Total parameters:     {model.count_params():,}\")\n",
    "print(f\"  Trainable:            {trainable:,}\")\n",
    "print(f\"  Non-trainable:        {non_trainable:,}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE_PHASE2),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Model recompiled for fine-tuning\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE_PHASE2} (10Ã— lower)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SETUP CALLBACKS - PHASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up callbacks for Phase 2...\\n\")\n",
    "\n",
    "# Callbacks for Phase 2\n",
    "callbacks_phase2 = [\n",
    "    ModelCheckpoint(\n",
    "        filepath=str(MODEL_DIR / 'efficientnet_b0_best.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=12,  # More patient for fine-tuning\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    CSVLogger(\n",
    "        str(OUTPUT_DIR / 'efficientnet_phase2_log.csv'),\n",
    "        append=True\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ… Callbacks configured:\")\n",
    "print(\"   - ModelCheckpoint: Save best model\")\n",
    "print(\"   - EarlyStopping: patience=12 (more patient)\")\n",
    "print(\"   - ReduceLROnPlateau: patience=5, factor=0.5\")\n",
    "print(\"   - CSVLogger: Log training metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. TRAIN - PHASE 2 (FINE-TUNING: ~40 EPOCHS, ~2-2.5 HOURS)\n",
    "\n",
    "**What's happening:**\n",
    "- ALL layers now trainable (base + top)\n",
    "- Lower learning rate (0.0001) for careful fine-tuning\n",
    "- This optimizes the entire network for waste classification\n",
    "- Expected time: ~2-2.5 hours with MPS\n",
    "\n",
    "**This is where the magic happens! ðŸŽ¯**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PHASE 2: FINE-TUNING (TRAIN ENTIRE MODEL)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Base model: UNFROZEN (all layers trainable)\")\n",
    "print(f\"Training: Entire model\")\n",
    "print(f\"Max epochs: {EPOCHS_PHASE2}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE_PHASE2} (10Ã— lower)\")\n",
    "print(f\"\\nExpected time: ~2-2.5 hours with MPS\")\n",
    "print(f\"              ~4-5 hours with CPU\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸš€ Starting Phase 2 training...\")\n",
    "print(\"   â˜• Grab a coffee/tea - this will take a while!\\n\")\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS_PHASE2,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_phase2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… PHASE 2 COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best val accuracy: {max(history_phase2.history['val_accuracy']):.4f}\")\n",
    "print(\"\\nModel saved: efficientnet_b0_best.h5\")\n",
    "print(\"\\nðŸŽ‰ EfficientNet training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. COMBINE TRAINING HISTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combining Phase 1 and Phase 2 histories...\\n\")\n",
    "\n",
    "# Combine histories\n",
    "combined_history = {\n",
    "    'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],\n",
    "    'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy'],\n",
    "    'loss': history_phase1.history['loss'] + history_phase2.history['loss'],\n",
    "    'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss'],\n",
    "    'lr': history_phase1.history.get('lr', [LEARNING_RATE_PHASE1] * len(history_phase1.history['loss'])) + \n",
    "          history_phase2.history.get('lr', [LEARNING_RATE_PHASE2] * len(history_phase2.history['loss']))\n",
    "}\n",
    "\n",
    "# Save combined history\n",
    "combined_df = pd.DataFrame({\n",
    "    'epoch': range(len(combined_history['accuracy'])),\n",
    "    'accuracy': combined_history['accuracy'],\n",
    "    'val_accuracy': combined_history['val_accuracy'],\n",
    "    'loss': combined_history['loss'],\n",
    "    'val_loss': combined_history['val_loss'],\n",
    "    'lr': combined_history['lr'],\n",
    "    'phase': ['Phase 1'] * len(history_phase1.history['loss']) + \n",
    "             ['Phase 2'] * len(history_phase2.history['loss'])\n",
    "})\n",
    "\n",
    "combined_df.to_csv(OUTPUT_DIR / 'efficientnet_training_log.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Combined training history saved\")\n",
    "print(f\"   Total epochs: {len(combined_history['accuracy'])}\")\n",
    "print(f\"   Phase 1: {len(history_phase1.history['loss'])} epochs\")\n",
    "print(f\"   Phase 2: {len(history_phase2.history['loss'])} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. PLOT TRAINING HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating training visualizations...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('EfficientNet-B0 Training History (Two-Phase)', fontsize=16, fontweight='bold')\n",
    "\n",
    "epochs = range(len(combined_history['accuracy']))\n",
    "phase1_end = len(history_phase1.history['loss'])\n",
    "\n",
    "# Plot 1: Accuracy\n",
    "ax = axes[0, 0]\n",
    "ax.plot(epochs, combined_history['accuracy'], 'b-', linewidth=2, label='Train')\n",
    "ax.plot(epochs, combined_history['val_accuracy'], 'r-', linewidth=2, label='Validation')\n",
    "ax.axvline(x=phase1_end, color='green', linestyle='--', linewidth=2, label='Phase 1â†’2')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate best\n",
    "best_val_acc = max(combined_history['val_accuracy'])\n",
    "best_epoch = combined_history['val_accuracy'].index(best_val_acc)\n",
    "ax.plot(best_epoch, best_val_acc, 'g*', markersize=15)\n",
    "ax.annotate(f'Best: {best_val_acc:.4f}', \n",
    "            xy=(best_epoch, best_val_acc),\n",
    "            xytext=(best_epoch + 2, best_val_acc - 0.05),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "            fontsize=10, fontweight='bold', color='green')\n",
    "\n",
    "# Plot 2: Loss\n",
    "ax = axes[0, 1]\n",
    "ax.plot(epochs, combined_history['loss'], 'b-', linewidth=2, label='Train')\n",
    "ax.plot(epochs, combined_history['val_loss'], 'r-', linewidth=2, label='Validation')\n",
    "ax.axvline(x=phase1_end, color='green', linestyle='--', linewidth=2, label='Phase 1â†’2')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning Rate\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epochs, combined_history['lr'], 'orange', linewidth=2)\n",
    "ax.axvline(x=phase1_end, color='green', linestyle='--', linewidth=2, label='Phase 1â†’2')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Summary\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "final_train_acc = combined_history['accuracy'][-1]\n",
    "final_val_acc = combined_history['val_accuracy'][-1]\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "EFFICIENTNET-B0 TRAINING SUMMARY\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "ARCHITECTURE:\n",
    "â”œâ”€ Base: EfficientNet-B0 (ImageNet)\n",
    "â”œâ”€ Parameters: ~5.3M\n",
    "â””â”€ Custom top layers: 3\n",
    "\n",
    "TRAINING STRATEGY:\n",
    "â”œâ”€ Phase 1: Freeze base, train top\n",
    "â”‚   â””â”€ Epochs: {len(history_phase1.history['loss'])}\n",
    "â”œâ”€ Phase 2: Fine-tune entire model\n",
    "â”‚   â””â”€ Epochs: {len(history_phase2.history['loss'])}\n",
    "â””â”€ Total epochs: {len(combined_history['accuracy'])}\n",
    "\n",
    "FINAL RESULTS:\n",
    "â”œâ”€ Train Accuracy:   {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\n",
    "â”œâ”€ Val Accuracy:     {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\n",
    "â””â”€ Best Val Acc:     {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\n",
    "\n",
    "STATUS: âœ… COMPLETE!\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
    "        family='monospace', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plot_path = OUTPUT_DIR / 'efficientnet_training_history.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"âœ… Plot saved: {plot_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "best_model = keras.models.load_model(MODEL_DIR / 'efficientnet_b0_best.h5')\n",
    "print(\"âœ… Best model loaded\\n\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_loss, val_accuracy = best_model.evaluate(val_generator, verbose=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Validation Loss:      {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy:  {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'model': 'EfficientNet-B0',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'phase1_epochs': len(history_phase1.history['loss']),\n",
    "    'phase2_epochs': len(history_phase2.history['loss']),\n",
    "    'total_epochs': len(combined_history['accuracy']),\n",
    "    'best_val_accuracy': float(best_val_acc),\n",
    "    'best_epoch': int(best_epoch),\n",
    "    'final_val_loss': float(val_loss),\n",
    "    'final_val_accuracy': float(val_accuracy),\n",
    "    'parameters': int(model.count_params())\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'efficientnet_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Results saved: efficientnet_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. COMPARE WITH BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL COMPARISON: BASELINE vs EFFICIENTNET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load baseline results\n",
    "baseline_log = pd.read_csv(OUTPUT_DIR / 'baseline_training_log.csv')\n",
    "baseline_best = baseline_log['val_accuracy'].max()\n",
    "\n",
    "# EfficientNet results\n",
    "efficient_best = best_val_acc\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = efficient_best - baseline_best\n",
    "improvement_pct = (improvement / baseline_best) * 100\n",
    "\n",
    "print(\"\\nRESULTS:\")\n",
    "print(f\"  Baseline CNN:       {baseline_best:.4f} ({baseline_best*100:.2f}%)\")\n",
    "print(f\"  EfficientNet-B0:    {efficient_best:.4f} ({efficient_best*100:.2f}%)\")\n",
    "print(f\"\\nIMPROVEMENT:\")\n",
    "print(f\"  Absolute:           +{improvement:.4f} (+{improvement*100:.2f}%)\")\n",
    "print(f\"  Relative:           +{improvement_pct:.2f}% improvement\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['Baseline CNN', 'EfficientNet-B0']\n",
    "accuracies = [baseline_best * 100, efficient_best * 100]\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "bars = ax.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add improvement arrow\n",
    "ax.annotate('', xy=(1, efficient_best * 100), xytext=(0, baseline_best * 100),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='red'))\n",
    "ax.text(0.5, (baseline_best * 100 + efficient_best * 100) / 2,\n",
    "        f'+{improvement*100:.2f}%\\nimprovement',\n",
    "        ha='center', va='center', fontsize=12, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "ax.set_ylabel('Validation Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Comparison: Baseline vs EfficientNet-B0', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 100])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Comparison plot saved: model_comparison.png\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ PHASE 4.2 COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nâœ… EfficientNet-B0 achieved {efficient_best*100:.2f}% validation accuracy\")\n",
    "print(f\"âœ… {improvement_pct:.1f}% improvement over baseline\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - models/efficientnet_b0_best.h5\")\n",
    "print(f\"  - outputs/efficientnet_training_log.csv\")\n",
    "print(f\"  - outputs/efficientnet_training_history.png\")\n",
    "print(f\"  - outputs/efficientnet_results.json\")\n",
    "print(f\"  - outputs/model_comparison.png\")\n",
    "print(\"\\nðŸš€ Ready for Phase 4.3: Model Comparison & Analysis!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (waste-m4)",
   "language": "python",
   "name": "waste-m4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
